{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Introduction </center>\n",
    "\n",
    "I chose to implement the Apriori algorithm to gain a better understanding of the theory behind it and to further explore the data mining and association rule generating algorithms that drive our modern digital world. I chose not to use any pre-implemented functions from the many robust data science packages that exist for Python, as doing so would negate the purpose of this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input \n",
    "\n",
    "This program is written to read in a CSV file of itemsets. These itemsets can be actual transactions such as milk, beer, diapers, or they can be groups of numbers such as 1,2,3,4. The CSV must not have a header and there can only be one transaction per line, for example:\\\n",
    "*<center> 1,2,3,4 </center>*\\\n",
    "*<center> 1,2,4 </center>*\\\n",
    "*<center> 1,2,3,4,5,6 </center>*\\\n",
    "*<center> 1,2,5,4 </center>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def ReadCSV(inputFile):\n",
    "    C1 = []\n",
    "    inputTextToList = []\n",
    "    numberOfLines = 0\n",
    "\n",
    "    with open(inputFile, \"r\") as f:\n",
    "        csvFile = csv.reader(f)\n",
    "        for line in csvFile:\n",
    "            inputTextToList.append(line)\n",
    "            for word in line:\n",
    "                tempWordList = []\n",
    "                tempWordList.append(word)\n",
    "                if tempWordList not in C1:\n",
    "                    C1.append(tempWordList)\n",
    "            numberOfLines += 1 \n",
    "    return inputTextToList, numberOfLines, C1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the file is read in, a copy of the file is stored in a list of lists. At the same time, the number of lines in the file is counted, as this is important for calculating the support of items. Finally, the first candidate set is created by appending the current item to a list every time a unique item is encountered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "\n",
    "Once C1 is generated and the minimum support is formulated as a percent, the candidate set can be pruned of infrequent items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PruneInfrequentItemsets(CK, minimumSupport, inputTextToList):\n",
    "    LK = []\n",
    "    for itemSet in CK:\n",
    "        if Support(itemSet, inputTextToList) >= minimumSupport:\n",
    "            LK.append(itemSet)\n",
    "    return LK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pruning is very simple. With the candidate set (CK), minimum support, and a copy of the file supplied, all that needs to be done is to compare the support of each itemset in the candidate set to the minimum support. If the support for an itemset is greater than or equal to the provided support, then the itemset is appended to LK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Support(itemSet, inputTextToList):\n",
    "    numberOfOccurences = 0\n",
    "    for line in inputTextToList:\n",
    "        if all(e in line for e in itemSet):\n",
    "            numberOfOccurences += 1\n",
    "    return numberOfOccurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating support is equally simple. If every item in an itemset occurs in an itemset in the original file, then the number of occurrences (support) is increased by one for that itemset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori\n",
    "\n",
    "Once C1 and subsequently L1 have been generated, the main loop of the algorithm is ready to be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LK = L1\n",
    "frequentItemSets = []\n",
    "K = 2\n",
    "    \n",
    "while LK:\n",
    "    frequentItemSets.extend(LK)\n",
    "    #generate possile combinations (candidate set)\n",
    "    CK = utils.GenerateCandidateSet(LK, K)\n",
    "    LK.clear()\n",
    "    #prune the infrequent items save to LK\n",
    "    LK = utils.PruneInfrequentItemsets(CK, minSupport, inputTextToList)\n",
    "    K += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this loop, the exact steps as described above occur in a similar order with one extra step, the generation of new candidate sets. First, LK is appended to a list of frequent itemsets. Next, a new candidate set is generated using the previous frequent itemsets. Then, LK is cleared so that the pruned CK can be saved and the process can start again, ending once there are no more itemsets that satisfy the minimum support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCandidateSet(itemset, K):\n",
    "    CK = []\n",
    "    requiredNumberOfMatchingElements = (K - 2)\n",
    "    for x in range(len(itemset)):\n",
    "        for y in range(x + 1, len(itemset)):\n",
    "            #if the first K - 2 elements are the same then make combination\n",
    "            if MatchingFirstKElements(itemset[x],itemset[y], requiredNumberOfMatchingElements):\n",
    "                #make a new list from joining x and y\n",
    "                newList = itemset[x] + itemset[y]\n",
    "                #remove duplicates\n",
    "                newList = list(dict.fromkeys(newList))\n",
    "                #append to CK\n",
    "                CK.append(newList)\n",
    "    return CK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating frequent itemsets from previously frequent itemsets, it is important to remember that two itemsets must have K - 2 matching first elements. For example, when generating C2, the itemsets [1] and [2] do not have to match; however, when generating C3, the itemsets [1, 2] and [1, 3] must match the first item to be combined.\n",
    "\n",
    "To generate candidate sets, every itemset (i) needs to be compared with every next itemset in the list (i+1). For each itemset, the number of matching first items needs to be checked. If the items match, then the itemsets can be combined. First, the itemsets are appended together; from there, the duplicate items are removed. Finally, this new itemset is added to the new candidate set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatchingFirstKElements(list1, list2, K):\n",
    "    for i in range(K):\n",
    "        if list1[i] != list2[i]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To match the first K - 2 items of two itemsets, the two itemsets must be iterated over for however large K is at that iteration of the loop. If at any point the two itemsets do not match, then the comparison can be stopped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Rules\n",
    "\n",
    "Once the frequent itemsets are generated, association rules can be derived from them. These association rules show how often one item or group of items occurs when another item is present. This can be useful in the real world as it can reveal hidden relationships between things, such as the tendency to buy beer when also buying diapers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def AssociationRules(frequentItemSets, inputTextToList, minConfidence):\n",
    "    associationRules = []\n",
    "    for item in frequentItemSets:\n",
    "        powerset = list(chain.from_iterable(combinations(item, r) for r in range(1,len(item))))\n",
    "        for set in powerset:\n",
    "            confidence = (Support(item, inputTextToList) / Support(list(set), inputTextToList))\n",
    "            if confidence >= minConfidence:\n",
    "                associationRules.append([list(set),[x for x in item if x not in list(set)], confidence])\n",
    "    return associationRules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate association rules, every itemset in the frequent itemsets must be broken down into a power set or a list of every possible combination of items in the itemset. With this power set, the confidence can be calculated using the support method from above. To do this, the support for the itemset the power set was generated from is divided by the support for each item in the power set. If the confidence for that item in the power set is greater than or equal to the minimum confidence, then it is added to the list of association rules."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
